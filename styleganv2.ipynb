{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"styleganv2.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPzTcZ2E5Ru5HzLE4sVyFFm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b93ae37b4c1442bf953d56d689b95226":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f370234517a3464397011b997a51b51b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c98e34d986514184a85d045c67108fd6","IPY_MODEL_ebbcca5e9ba94350addd11643ee91d6e","IPY_MODEL_40c87f165340406eb1c4dcb6cf26d441"]}},"f370234517a3464397011b997a51b51b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c98e34d986514184a85d045c67108fd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_557a1eb40d724c8d84d3f74349434148","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce4d074fd53d49f08f8d78b2bfffea73"}},"ebbcca5e9ba94350addd11643ee91d6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b241a5f3d8d74297b637e23b58dbc24d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_64fee464fe824a8daeb293e3877e196c"}},"40c87f165340406eb1c4dcb6cf26d441":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_28382333ecc7423ea12fafa02737a53f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/4 [00:02&lt;00:00,  1.72it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_099856a6f499413b99cb9f3c4f1df6d9"}},"557a1eb40d724c8d84d3f74349434148":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ce4d074fd53d49f08f8d78b2bfffea73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b241a5f3d8d74297b637e23b58dbc24d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"64fee464fe824a8daeb293e3877e196c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"28382333ecc7423ea12fafa02737a53f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"099856a6f499413b99cb9f3c4f1df6d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7NdiTBt-qmc","executionInfo":{"status":"ok","timestamp":1629786997139,"user_tz":-540,"elapsed":910,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"1505c542-b047-4570-a878-d33d611aee4c"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Tue Aug 24 06:36:36 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sgoRr1ICGZz","executionInfo":{"status":"ok","timestamp":1629787023883,"user_tz":-540,"elapsed":26766,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"11a22efb-ed7b-4a81-b1d4-f6acd5e68988"},"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    COLAB = True\n","    print(\"Note: using Google CoLab\")\n","except:\n","    print(\"Note: not using Google CoLab\")\n","    COLAB = False"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Note: using Google CoLab\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8OK4t9sCWMm","executionInfo":{"status":"ok","timestamp":1629787029629,"user_tz":-540,"elapsed":5756,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"e505652e-09f5-4bdf-a45a-8f7c4b9cf9d9"},"source":["!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n","!pip install ninja"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'stylegan2-ada-pytorch'...\n","remote: Enumerating objects: 125, done.\u001b[K\n","remote: Total 125 (delta 0), reused 0 (delta 0), pack-reused 125\u001b[K\n","Receiving objects: 100% (125/125), 1.12 MiB | 3.69 MiB/s, done.\n","Resolving deltas: 100% (55/55), done.\n","Collecting ninja\n","  Downloading ninja-1.10.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n","\u001b[K     |████████████████████████████████| 108 kB 4.0 MB/s \n","\u001b[?25hInstalling collected packages: ninja\n","Successfully installed ninja-1.10.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PWT_KyvLCdPo","executionInfo":{"status":"ok","timestamp":1629787030165,"user_tz":-540,"elapsed":542,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"db308c21-1d14-4fc5-f460-aa5e3d56c774"},"source":["!ls /content/drive/MyDrive/final_project/data/images"],"execution_count":4,"outputs":[{"output_type":"stream","text":["seed0085.png  seed0265.png  seed0297.png  seed0849.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ga2S8zVBDpK1","executionInfo":{"status":"ok","timestamp":1629787030615,"user_tz":-540,"elapsed":453,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"b5df8f5e-ab0c-4db0-82d3-d8dd7daaa52a"},"source":["!python /content/stylegan2-ada-pytorch/dataset_tool.py --source /content/drive/MyDrive/final_project/data/images --dest /content/drive/MyDrive/final_project/data/dataset"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Error: --dest folder must be empty\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TecCEx6gGM5c","executionInfo":{"status":"ok","timestamp":1629787030616,"user_tz":-540,"elapsed":3,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}}},"source":["#!rm -R /content/drive/MyDrive/final_project/data/dataset/*"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":48,"referenced_widgets":["b93ae37b4c1442bf953d56d689b95226","f370234517a3464397011b997a51b51b","c98e34d986514184a85d045c67108fd6","ebbcca5e9ba94350addd11643ee91d6e","40c87f165340406eb1c4dcb6cf26d441","557a1eb40d724c8d84d3f74349434148","ce4d074fd53d49f08f8d78b2bfffea73","b241a5f3d8d74297b637e23b58dbc24d","64fee464fe824a8daeb293e3877e196c","28382333ecc7423ea12fafa02737a53f","099856a6f499413b99cb9f3c4f1df6d9"]},"id":"jSCE0HQLGfz6","executionInfo":{"status":"ok","timestamp":1629787033348,"user_tz":-540,"elapsed":2735,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"cf7ec5eb-6e13-4070-e3ae-6d0b2bbec7b8"},"source":["from os import listdir\n","from os.path import isfile, join\n","import os\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","\n","IMAGE_PATH = '/content/drive/MyDrive/final_project/data/images'\n","files = [f for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))]\n","\n","base_size = None\n","for file in tqdm(files):\n","  file2 = os.path.join(IMAGE_PATH,file)\n","  img = Image.open(file2)\n","  sz = img.size\n","  if base_size and sz!=base_size:\n","    print(f\"Inconsistant size: {file2}\")\n","  elif img.mode!='RGB':\n","    print(f\"Inconsistant color format: {file2}\")\n","  else:\n","    base_size = sz"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b93ae37b4c1442bf953d56d689b95226","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"845Rq5AKGnUg","executionInfo":{"status":"ok","timestamp":1629787233340,"user_tz":-540,"elapsed":200005,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"b7ab56e5-1ba2-44a7-f8dd-04063b07604d"},"source":["import os\n","\n","# Modify these to suit your needs\n","EXPERIMENTS = \"/content/drive/MyDrive/final_project/data/experiments\"\n","DATA = \"/content/drive/MyDrive/final_project/data/dataset\"\n","SNAP = 10\n","\n","# Build the command and run it\n","cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --outdir {EXPERIMENTS} --data {DATA}\"\n","!{cmd}"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 10,\n","  \"network_snapshot_ticks\": 10,\n","  \"metrics\": [\n","    \"fid50k_full\"\n","  ],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/content/drive/MyDrive/final_project/data/dataset\",\n","    \"use_labels\": false,\n","    \"max_size\": 4,\n","    \"xflip\": false,\n","    \"resolution\": 1024\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 2\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 32768,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 4\n","    },\n","    \"channel_base\": 32768,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.002,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.002,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 52.4288\n","  },\n","  \"total_kimg\": 25000,\n","  \"batch_size\": 4,\n","  \"batch_gpu\": 4,\n","  \"ema_kimg\": 1.25,\n","  \"ema_rampup\": 0.05,\n","  \"ada_target\": 0.6,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1,\n","    \"brightness\": 1,\n","    \"contrast\": 1,\n","    \"lumaflip\": 1,\n","    \"hue\": 1,\n","    \"saturation\": 1\n","  },\n","  \"run_dir\": \"/content/drive/MyDrive/final_project/data/experiments/00002-dataset-auto1\"\n","}\n","\n","Output directory:   /content/drive/MyDrive/final_project/data/experiments/00002-dataset-auto1\n","Training data:      /content/drive/MyDrive/final_project/data/dataset\n","Training duration:  25000 kimg\n","Number of GPUs:     1\n","Number of images:   4\n","Image resolution:   1024\n","Conditional model:  False\n","Dataset x-flips:    False\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","Num images:  4\n","Image shape: [3, 1024, 1024]\n","Label shape: [0]\n","\n","Constructing networks...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","\n","Generator              Parameters  Buffers  Output shape         Datatype\n","---                    ---         ---      ---                  ---     \n","mapping.fc0            262656      -        [4, 512]             float32 \n","mapping.fc1            262656      -        [4, 512]             float32 \n","mapping                -           512      [4, 18, 512]         float32 \n","synthesis.b4.conv1     2622465     32       [4, 512, 4, 4]       float32 \n","synthesis.b4.torgb     264195      -        [4, 3, 4, 4]         float32 \n","synthesis.b4:0         8192        16       [4, 512, 4, 4]       float32 \n","synthesis.b4:1         -           -        [4, 512, 4, 4]       float32 \n","synthesis.b8.conv0     2622465     80       [4, 512, 8, 8]       float32 \n","synthesis.b8.conv1     2622465     80       [4, 512, 8, 8]       float32 \n","synthesis.b8.torgb     264195      -        [4, 3, 8, 8]         float32 \n","synthesis.b8:0         -           16       [4, 512, 8, 8]       float32 \n","synthesis.b8:1         -           -        [4, 512, 8, 8]       float32 \n","synthesis.b16.conv0    2622465     272      [4, 512, 16, 16]     float32 \n","synthesis.b16.conv1    2622465     272      [4, 512, 16, 16]     float32 \n","synthesis.b16.torgb    264195      -        [4, 3, 16, 16]       float32 \n","synthesis.b16:0        -           16       [4, 512, 16, 16]     float32 \n","synthesis.b16:1        -           -        [4, 512, 16, 16]     float32 \n","synthesis.b32.conv0    2622465     1040     [4, 512, 32, 32]     float32 \n","synthesis.b32.conv1    2622465     1040     [4, 512, 32, 32]     float32 \n","synthesis.b32.torgb    264195      -        [4, 3, 32, 32]       float32 \n","synthesis.b32:0        -           16       [4, 512, 32, 32]     float32 \n","synthesis.b32:1        -           -        [4, 512, 32, 32]     float32 \n","synthesis.b64.conv0    2622465     4112     [4, 512, 64, 64]     float32 \n","synthesis.b64.conv1    2622465     4112     [4, 512, 64, 64]     float32 \n","synthesis.b64.torgb    264195      -        [4, 3, 64, 64]       float32 \n","synthesis.b64:0        -           16       [4, 512, 64, 64]     float32 \n","synthesis.b64:1        -           -        [4, 512, 64, 64]     float32 \n","synthesis.b128.conv0   1442561     16400    [4, 256, 128, 128]   float16 \n","synthesis.b128.conv1   721409      16400    [4, 256, 128, 128]   float16 \n","synthesis.b128.torgb   132099      -        [4, 3, 128, 128]     float16 \n","synthesis.b128:0       -           16       [4, 256, 128, 128]   float16 \n","synthesis.b128:1       -           -        [4, 256, 128, 128]   float32 \n","synthesis.b256.conv0   426369      65552    [4, 128, 256, 256]   float16 \n","synthesis.b256.conv1   213249      65552    [4, 128, 256, 256]   float16 \n","synthesis.b256.torgb   66051       -        [4, 3, 256, 256]     float16 \n","synthesis.b256:0       -           16       [4, 128, 256, 256]   float16 \n","synthesis.b256:1       -           -        [4, 128, 256, 256]   float32 \n","synthesis.b512.conv0   139457      262160   [4, 64, 512, 512]    float16 \n","synthesis.b512.conv1   69761       262160   [4, 64, 512, 512]    float16 \n","synthesis.b512.torgb   33027       -        [4, 3, 512, 512]     float16 \n","synthesis.b512:0       -           16       [4, 64, 512, 512]    float16 \n","synthesis.b512:1       -           -        [4, 64, 512, 512]    float32 \n","synthesis.b1024.conv0  51297       1048592  [4, 32, 1024, 1024]  float16 \n","synthesis.b1024.conv1  25665       1048592  [4, 32, 1024, 1024]  float16 \n","synthesis.b1024.torgb  16515       -        [4, 3, 1024, 1024]   float16 \n","synthesis.b1024:0      -           16       [4, 32, 1024, 1024]  float16 \n","synthesis.b1024:1      -           -        [4, 32, 1024, 1024]  float32 \n","---                    ---         ---      ---                  ---     \n","Total                  28794124    2797104  -                    -       \n","\n","\n","Discriminator  Parameters  Buffers  Output shape         Datatype\n","---            ---         ---      ---                  ---     \n","b1024.fromrgb  128         16       [4, 32, 1024, 1024]  float16 \n","b1024.skip     2048        16       [4, 64, 512, 512]    float16 \n","b1024.conv0    9248        16       [4, 32, 1024, 1024]  float16 \n","b1024.conv1    18496       16       [4, 64, 512, 512]    float16 \n","b1024          -           16       [4, 64, 512, 512]    float16 \n","b512.skip      8192        16       [4, 128, 256, 256]   float16 \n","b512.conv0     36928       16       [4, 64, 512, 512]    float16 \n","b512.conv1     73856       16       [4, 128, 256, 256]   float16 \n","b512           -           16       [4, 128, 256, 256]   float16 \n","b256.skip      32768       16       [4, 256, 128, 128]   float16 \n","b256.conv0     147584      16       [4, 128, 256, 256]   float16 \n","b256.conv1     295168      16       [4, 256, 128, 128]   float16 \n","b256           -           16       [4, 256, 128, 128]   float16 \n","b128.skip      131072      16       [4, 512, 64, 64]     float16 \n","b128.conv0     590080      16       [4, 256, 128, 128]   float16 \n","b128.conv1     1180160     16       [4, 512, 64, 64]     float16 \n","b128           -           16       [4, 512, 64, 64]     float16 \n","b64.skip       262144      16       [4, 512, 32, 32]     float32 \n","b64.conv0      2359808     16       [4, 512, 64, 64]     float32 \n","b64.conv1      2359808     16       [4, 512, 32, 32]     float32 \n","b64            -           16       [4, 512, 32, 32]     float32 \n","b32.skip       262144      16       [4, 512, 16, 16]     float32 \n","b32.conv0      2359808     16       [4, 512, 32, 32]     float32 \n","b32.conv1      2359808     16       [4, 512, 16, 16]     float32 \n","b32            -           16       [4, 512, 16, 16]     float32 \n","b16.skip       262144      16       [4, 512, 8, 8]       float32 \n","b16.conv0      2359808     16       [4, 512, 16, 16]     float32 \n","b16.conv1      2359808     16       [4, 512, 8, 8]       float32 \n","b16            -           16       [4, 512, 8, 8]       float32 \n","b8.skip        262144      16       [4, 512, 4, 4]       float32 \n","b8.conv0       2359808     16       [4, 512, 8, 8]       float32 \n","b8.conv1       2359808     16       [4, 512, 4, 4]       float32 \n","b8             -           16       [4, 512, 4, 4]       float32 \n","b4.mbstd       -           -        [4, 513, 4, 4]       float32 \n","b4.conv        2364416     16       [4, 512, 4, 4]       float32 \n","b4.fc          4194816     -        [4, 512]             float32 \n","b4.out         513         -        [4, 1]               float32 \n","---            ---         ---      ---                  ---     \n","Total          29012513    544      -                    -       \n","\n","Setting up augmentation...\n","Distributing across 1 GPUs...\n","Setting up training phases...\n","Exporting sample images...\n","Initializing logs...\n","Training for 25000 kimg...\n","\n","tick 0     kimg 0.0      time 2m 29s       sec/tick 45.0    sec/kimg 11241.77 maintenance 104.3  cpumem 3.52   gpumem 10.05  augment 0.000\n","Evaluating metrics...\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return forward_call(*input, **kwargs)\n","/usr/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 17 leaked semaphores to clean up at shutdown\n","  len(cache))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHTdQSJsGxtq","executionInfo":{"status":"ok","timestamp":1629787243156,"user_tz":-540,"elapsed":9824,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"a9f51dde-d17c-4496-bb1f-eb9980be5820"},"source":["!/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap 25 --resume /content/drive/MyDrive/final_project/data/experiments/00007-circuit-auto1/network-snapshot-000500.pkl --outdir /content/drive/MyDrive/final_project/data/experiments --data /content/drive/MyDrive/final_project/data/dataset"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 25,\n","  \"network_snapshot_ticks\": 25,\n","  \"metrics\": [\n","    \"fid50k_full\"\n","  ],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/content/drive/MyDrive/final_project/data/dataset\",\n","    \"use_labels\": false,\n","    \"max_size\": 4,\n","    \"xflip\": false,\n","    \"resolution\": 1024\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 2\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 32768,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 4\n","    },\n","    \"channel_base\": 32768,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.002,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.002,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 52.4288\n","  },\n","  \"total_kimg\": 25000,\n","  \"batch_size\": 4,\n","  \"batch_gpu\": 4,\n","  \"ema_kimg\": 1.25,\n","  \"ema_rampup\": null,\n","  \"ada_target\": 0.6,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1,\n","    \"brightness\": 1,\n","    \"contrast\": 1,\n","    \"lumaflip\": 1,\n","    \"hue\": 1,\n","    \"saturation\": 1\n","  },\n","  \"resume_pkl\": \"/content/drive/MyDrive/final_project/data/experiments/00007-circuit-auto1/network-snapshot-000500.pkl\",\n","  \"ada_kimg\": 100,\n","  \"run_dir\": \"/content/drive/MyDrive/final_project/data/experiments/00003-dataset-auto1-resumecustom\"\n","}\n","\n","Output directory:   /content/drive/MyDrive/final_project/data/experiments/00003-dataset-auto1-resumecustom\n","Training data:      /content/drive/MyDrive/final_project/data/dataset\n","Training duration:  25000 kimg\n","Number of GPUs:     1\n","Number of images:   4\n","Image resolution:   1024\n","Conditional model:  False\n","Dataset x-flips:    False\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","Num images:  4\n","Image shape: [3, 1024, 1024]\n","Label shape: [0]\n","\n","Constructing networks...\n","Resuming from \"/content/drive/MyDrive/final_project/data/experiments/00007-circuit-auto1/network-snapshot-000500.pkl\"\n","Traceback (most recent call last):\n","  File \"/content/stylegan2-ada-pytorch/train.py\", line 538, in <module>\n","    main() # pylint: disable=no-value-for-parameter\n","  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n","    return self.main(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n","    rv = self.invoke(ctx)\n","  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n","    return ctx.invoke(self.callback, **ctx.params)\n","  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n","    return callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/click/decorators.py\", line 21, in new_func\n","    return f(get_current_context(), *args, **kwargs)\n","  File \"/content/stylegan2-ada-pytorch/train.py\", line 531, in main\n","    subprocess_fn(rank=0, args=args, temp_dir=temp_dir)\n","  File \"/content/stylegan2-ada-pytorch/train.py\", line 383, in subprocess_fn\n","    training_loop.training_loop(rank=rank, **args)\n","  File \"/content/stylegan2-ada-pytorch/training/training_loop.py\", line 157, in training_loop\n","    with dnnlib.util.open_url(resume_pkl) as f:\n","  File \"/content/stylegan2-ada-pytorch/dnnlib/util.py\", line 389, in open_url\n","    return url if return_filename else open(url, \"rb\")\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/final_project/data/experiments/00007-circuit-auto1/network-snapshot-000500.pkl'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-5o4l2bHIiCA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629787244911,"user_tz":-540,"elapsed":1769,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"75c026cd-b88b-499c-f0e8-f90081d23215"},"source":["import os\n","\n","# Modify these to suit your needs\n","EXPERIMENTS = \"/content/drive/MyDrive/data/gan/experiments\"\n","NETWORK = \"network-snapshot-000100.pkl\"\n","RESUME = os.path.join(EXPERIMENTS, \"00008-circuit-auto1-resumecustom\", NETWORK)\n","DATA = \"/content/drive/MyDrive/data/gan/dataset/circuit\"\n","SNAP = 10\n","\n","# Build the command and run it\n","cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --resume {RESUME} --outdir {EXPERIMENTS} --data {DATA}\"\n","!{cmd}"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Usage: train.py [OPTIONS]\n","Try 'train.py --help' for help.\n","\n","Error: --data: Path must point to a directory or zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9V9jmJfLnve","executionInfo":{"status":"ok","timestamp":1629787259461,"user_tz":-540,"elapsed":14562,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"82adbad7-befe-410b-a265-8b2efb0f35f1"},"source":["!python /content/stylegan2-ada-pytorch/generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n","    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Loading networks from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\"...\n","Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ... done\n","Generating image for seed 85 (0/4) ...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","Generating image for seed 265 (1/4) ...\n","Generating image for seed 297 (2/4) ...\n","Generating image for seed 849 (3/4) ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Ds8zUlfL1RJ","executionInfo":{"status":"ok","timestamp":1629787302579,"user_tz":-540,"elapsed":43128,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}},"outputId":"89a134b9-ce07-4746-fccf-69193e8094b7"},"source":["!python /content/stylegan2-ada-pytorch/style_mixing.py --outdir=out --rows=85,100,75,458,1500 --cols=55,821,1789,293 \\\n","    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Loading networks from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\"...\n","Generating W vectors...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Generating images...\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","Generating style-mixed images...\n","Saving images...\n","Saving image grid...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CKoDA-uQMsWd","executionInfo":{"status":"ok","timestamp":1629787302580,"user_tz":-540,"elapsed":34,"user":{"displayName":"황동준","photoUrl":"","userId":"14467842063552825644"}}},"source":[""],"execution_count":12,"outputs":[]}]}